# -*- coding: utf-8 -*-
"""Momentum metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-fJDibwordnp6lpV6TUr_NjAdoPwIfW
"""



"""# 📊 Momentum Metrics EDA & Interview Prep

This notebook walks through loading, cleaning, exploring, and modeling the momentum metrics dataset used by Bloom.  
Each section includes mentor-style explanations to help juniors follow the logic clearly.

## 1. Loading & Overview of Dataset

**Goal:** Load the dataset and get a feel for its shape and structure.

We start by importing pandas, reading the CSV, and checking dimensions.
"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)

import pandas as pd
import numpy as np

# Load the dirty dataset with missing values, duplicates, and outliers
df = pd.read_csv("/content/drive/MyDrive/Momentum metrics/momentum_metrics_dirty.csv")

# Print the shape (rows, columns) — basic sanity check
print(f"Dataset shape: {df.shape}")

# Preview first few rows
df.head()

"""
## 2. Data Cleaning — Handling Missing Values and Duplicates

**Goal:** Identify missing data and duplicates, then handle them appropriately.

- `.isna().sum()` shows how many missing values per column.
- `.duplicated().sum()` counts exact duplicate rows.

-Why use median and not mean?
Mean is strongly affected by any extreme values(too small/large values) in data--- skews the distribution

Median-- is the middle value of a sorted data.  It is much reliable indicator of data distribution.
"""

# Check missing values count per column
print("Missing values per column:")
print(df.isna().sum())

# Count duplicate rows
print(f"Number of duplicate rows: {df.duplicated().sum()}")

# Fill missing numeric values with median (to avoid bias from outliers)
df['days_to_complete'].fillna(df['days_to_complete'].median(), inplace=True)
df['checkins_logged'].fillna(df['checkins_logged'].median(), inplace=True)

# Fill missing binary attendance with mode (most frequent value)
df['meeting_attended'].fillna(df['meeting_attended'].mode()[0], inplace=True)

# Drop exact duplicate rows-- permanently --- inplace=True
df.drop_duplicates(inplace=True)

# Confirm changes--- printing results in a presentable manner.
print("After cleaning:")
print(df.isna().sum())
print(f"Duplicates remaining: {df.duplicated().sum()}")

"""## 3. Exploratory Data Analysis — Descriptive Statistics and Outliers

**Goal:** Understand distributions, spot extreme values, and detect possible outliers.

- `.describe()` gives summary stats for numeric columns.
- Filter values that exceed reasonable thresholds as defined by the company policies.

"""

# Summary statistics
print(df.describe())

# Check for outliers in checkins_logged and engagement_score
outliers = df[(df['checkins_logged'] > 15) | (df['engagement_score'] > 100)]

print(f"Outlier rows found: {len(outliers)}")
print(outliers[['user_id', 'checkins_logged', 'engagement_score']])

"""## 4. Visual Explorations — Histograms and Correlation Heatmap

**Goal:** Visualize distributions and relationships to inform feature engineering and modeling.

- `hist()` shows distribution shape.
- `sns.heatmap()` highlights correlation strength between variables.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histograms of numeric columns
df.hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

# Correlation heatmap between numeric columns
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""| Pair                                    | Correlation | Interpretation                                                                                 |
| --------------------------------------- | ----------- | ---------------------------------------------------------------------------------------------- |
| `meeting_attended` & `engagement_score` | **0.71**    | Strong positive  — attending meetings is highly associated with higher engagement.           |
| `checkins_logged` & `engagement_score`  | **0.67**    | Strong positive — more check-ins → more engagement.                                            |
| `days_to_complete` & `engagement_score` | **-0.26**   | Weak-to-moderate **negative** — taking longer to complete tasks is linked to lower engagement. |
| `meeting_attended` & `checkins_logged`  | **0.092**   | Weak positive — small overlap between checking in and attending meetings.                      |
| Other values (near 0)                   | ≈ 0         | No meaningful correlation — random noise or unrelated features.                                |


Summary

engagement score is strongly correlated with both meeting attendance and check-ins.

It’s negatively correlated with the time taken to complete tasks.

Other features like record_id don’t matter --index.

---

##  Feature Engineering — Capturing Engagement Patterns

###  Goal:

Create new features that better capture user engagement patterns.

---

###  What is Feature Engineering?

**Feature engineering** means creating new columns (features) from existing ones to uncover deeper insights or improve model performance.

It’s essential for data analysis and machine learning.

🛠 **Example:**
If you have a column `year_built`, you can create a new column `house_age = current_year - year_built`.

---

###  Feature: `checkins_per_day`

We want to normalize check-ins by task duration to account for users who completed tasks faster or slower.

```python
df['checkins_per_day'] = df['checkins_logged'] / df['days_to_complete']
```

>  Be careful of division by zero or missing values!
> Handle this with `.fillna()` or conditional logic before creating the feature.

---

###  Quick Recap: Correlation

**Correlation** tells us how two variables change together — especially in a linear way.

| Type         | Meaning                            |
| ------------ | --------------------------------- |
| **Negative** | As X increases, Y decreases  |
| **Zero**     | No consistent pattern          |

>  **Note:** Correlation ≠ Causation
> Just because two variables move together doesn’t mean one causes the other.

---

###  Summary for Juniors:

* Feature engineering helps you get more out of your data.
* `checkins_per_day` is a useful normalized metric.
* Always handle missing or risky values before creating new columns.
* Use correlation to explore relationships — but don’t jump to causal conclusions.

---
"""

df['checkins_per_day'] = df['checkins_logged'] / df['days_to_complete']
df['checkins_per_day'].fillna(0, inplace=True)  # Replace NaNs from division by zero
print(df[['checkins_per_day']].describe())

"""---

##  6. Modeling — Predicting Engagement Score Using Simple Linear Regression

###  Goal:

Understand how **check-ins**, **meeting attendance**, and **days to complete** impact **engagement**.

---

###  What is Simple Linear Regression?

Simple Linear Regression models the relationship between a **dependent variable (Y)** and one or more **independent variables (X)** by fitting the best possible straight line through the data.

* **Y (target):** `engagement_score`
* **X (features):** `checkins_logged`, `meeting_attended`, `days_to_complete`

---

###  Tool Used: `sklearn.linear_model.LinearRegression`

Scikit-learn is a popular Python library for machine learning, and `LinearRegression()` helps us fit a simple linear model.

---

###  Model Validation: Train-Test Split

To check how well the model performs on **unseen data**, we use a **train-test split**:

* **Training set** → Like lecture notes. Used to "teach" the model.
* **Testing set** → Like the exam. Used to see how well the model "remembers."

---

###  Evaluation Metric: RMSE (Root Mean Squared Error)

* RMSE measures the average prediction error — how far off the model's predictions are from actual values.
* It’s in the **same units** as the target variable, which makes it easy to interpret.
* **Lower RMSE = better model performance.**
* Range: 0 (perfect) → ∞ (lots of error)

---

###  Summary for Juniors:

* **Target (Y):** `engagement_score`
* **Predictors (X):** `checkins_logged`, `meeting_attended`, `days_to_complete`
* Use `train_test_split()` to simulate real-world model use.
* Use `LinearRegression()` to train the model.
* Evaluate using **RMSE** to understand prediction accuracy.

---


"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Prepare features and target variable
X = df[['checkins_logged', 'meeting_attended', 'days_to_complete']]
y = df['engagement_score']

# Train-test split (random 75-25)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Initialize and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Calculate RMSE (root mean squared error)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Model RMSE: {rmse:.2f}")

"""## 7. Insights — Top Users and Team Performance

**Goal:** Summarize engagement scores by users and teams to find top performers.
Using pandas
- `.groupby()` aggregates data by user or team.
- Sort and display top 5 in each category.

"""

# Top 5 users by average engagement score
top_users = df.groupby('user_id')['engagement_score'].mean().sort_values(ascending=False).head(5)
print("Top 5 Users by Engagement Score:")
print(top_users)

# Average engagement by team
team_engagement = df.groupby('team_id')['engagement_score'].mean().sort_values(ascending=False)
print("\nTeam Average Engagement Scores:")
print(team_engagement)

"""Is 5.89 good or bad?
Let’s compare that to the range of engagement_score in our dataset.
"""

print(df['engagement_score'].min(), df['engagement_score'].max(), df['engagement_score'].std())

"""Our baseline linear model achieves an RMSE of ~5.89, which is acceptable given the engagement score range and the presence of mild outliers.
Future iterations can improve performance via feature expansion, non-linear modeling, or outlier handling — but for this exploratory pass, the model provides decent signal.

## 8. Time Series — Weekly Engagement Trend for Team 1

**Goal:** Visualize how engagement score changes over time for one team, using weekly averages.

This helps spot momentum shifts (up or down), without over-interpreting short-term noise.
"""

# Convert to datetime (again, just in case)
df['task_created'] = pd.to_datetime(df['task_created'])

# Filter to Team 1 and sort
team_df = df[df['team_id'] == 'team_1'].copy()
team_df.sort_values('task_created', inplace=True)

# Resample weekly and forward-fill missing weeks
weekly_engagement = (
    team_df.set_index('task_created')
    .resample('W')['engagement_score']
    .mean()
    .fillna(method='ffill')  # forward-fill to prevent line gaps
)

# Plot
plt.figure(figsize=(10, 5))
weekly_engagement.plot(marker='o', color='teal')
plt.title("Weekly Avg Engagement — Team 1")
plt.ylabel("Engagement Score")
plt.xlabel("Week")
plt.grid(True)
plt.tight_layout()
plt.show()

"""## 📊 Final Observation: Team 1 Engagement Over Time

This weekly time series shows how Team 1's average engagement score evolved across 2024.

- **January to March:** Engagement fluctuated between ~47-62.
- **Late March:** A sharp drop to ~30 suggests a disruption — possibly missed meetings, fewer check-ins, or loss of momentum.
- **April onward:** Strong recovery followed by a sustained plateau around **65**, showing the team regained and maintained stable engagement through spring and summer.

 **Takeaway:** While short-term dips happen, Team 1 appears resilient and consistent over time. This kind of view can help surface when teams need support — or when they're thriving and don't need intervention.

"""



"""Hypothesis Testing
Simple A/B Test: Meeting Attendance vs. Engagement
Do people who attend meetings have higher engagement than those who don’t?


"""

from scipy.stats import ttest_ind

attended = df[df['meeting_attended'] == 1]['engagement_score'].dropna()
skipped = df[df['meeting_attended'] == 0]['engagement_score'].dropna()

# Run Welch's t-test
t_stat, p_val = ttest_ind(attended, skipped, equal_var=False)

print(f"T-statistic: {t_stat:.2f}")
print(f"P-value: {p_val:.4f}")

"""##  Mini Hypothesis Test: Meetings vs. Engagement

**Question:** Do people who attend meetings have higher engagement scores?

- H₀ (null): No difference in scores
- Hₐ (alt): Attending meetings → higher engagement
- Test: Welch’s t-test -perfect for comparing the average of two groups, especially if their sizes or variances differ.

**Result:**  
T = 22.80  
P = 0.00

If p-value	Interpretation
< 0.05	The difference is statistically significant — reject H₀
≥ 0.05	Not significant — we fail to reject H₀ (no strong evidence of a difference).


Interpretation:
With p < 0.05, we reject the null hypothesis and conclude meeting attendees have significantly higher engagement.

Summary:
The statistical test confirms that meeting attendees have significantly higher engagement scores than non-attendees (T = 22.80, p < 0.05).



"""

